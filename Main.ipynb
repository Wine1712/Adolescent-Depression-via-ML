{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Standard Libraries ===\n",
    "import os  # For file and directory operations\n",
    "import warnings  # To suppress unnecessary warnings\n",
    "\n",
    "# === Data Handling ===\n",
    "import pandas as pd  # For data manipulation using DataFrames\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# === Plotting and Visualization ===\n",
    "import matplotlib.pyplot as plt  # Basic plotting\n",
    "import seaborn as sns  # Beautiful statistical plots\n",
    "\n",
    "# === Preprocessing & Scaling ===\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize  # Feature scaling and label conversion\n",
    "\n",
    "# === Dimensionality Reduction ===\n",
    "from sklearn.decomposition import PCA  # For visualizing data in 2D using PCA\n",
    "\n",
    "# === Model Evaluation Metrics ===\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "\n",
    "# === Data Splitting ===\n",
    "from sklearn.model_selection import train_test_split  # For splitting train and test sets\n",
    "\n",
    "# === Machine Learning Models ===\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier  # Boosted trees\n",
    "\n",
    "# === Optimization Algorithms (Custom Implementations) ===\n",
    "from Optimizer.jasa_optimizer import JASAOptimizer\n",
    "from Optimizer.dhoa_sa_optimizer import DHOA_SA_Optimizer\n",
    "from Optimizer.iwo_sa_optimizer import IWO_SA_Optimizer\n",
    "from Optimizer.cos_optimizer import COSOptimizer\n",
    "from Optimizer.coiwso_sa_optimizer import COIWSO_SA_Optimizer\n",
    "\n",
    "# === Miscellaneous Utilities ===\n",
    "import inspect  # For inspecting Python objects (e.g., function signatures)\n",
    "\n",
    "# 🔇 Ignore any warnings to keep the notebook output clean\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f72ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot: PCA Decision Boundary ===\n",
    "def plot_pca_decision_boundary(X, y, model, title, save_dir):\n",
    "    \"\"\"\n",
    "    Projects data to 2D using PCA and plots the decision boundary of the given model.\n",
    "\n",
    "    Parameters:\n",
    "    - X : np.ndarray\n",
    "        Feature matrix (original, high-dimensional)\n",
    "    - y : np.ndarray\n",
    "        Corresponding class labels\n",
    "    - model : sklearn classifier\n",
    "        A scikit-learn style model (must have .fit and .predict)\n",
    "    - title : str\n",
    "        Title for the plot and filename\n",
    "    - save_dir : str\n",
    "        Directory to save the resulting PNG file\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Reduce feature space to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    # Step 2: Train model on PCA-transformed data\n",
    "    model.fit(X_pca, y)\n",
    "\n",
    "    # Step 3: Generate mesh grid for plotting the decision boundary\n",
    "    x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                         np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "    # Step 4: Predict across the mesh to get class labels\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Step 5: Plot the decision boundary and the data points\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)  # Soft background\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='k', s=20)  # Points\n",
    "    plt.title(f\"PCA Decision Boundary: {title}\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Step 6: Save the figure\n",
    "    plt.savefig(os.path.join(save_dir, f\"PCA_{title}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aadfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plot: Precision-Recall Curve ===\n",
    "def plot_precision_recall(y_test, y_probs, model_name, save_dir):\n",
    "    \"\"\"\n",
    "    Plots the Precision-Recall Curve for a binary classifier and saves it as a PNG.\n",
    "\n",
    "    Parameters:\n",
    "    - y_test : array-like\n",
    "        True binary labels (0 or 1)\n",
    "    - y_probs : array-like\n",
    "        Predicted probabilities for the positive class (class 1)\n",
    "    - model_name : str\n",
    "        Name of the model (used in plot title and filename)\n",
    "    - save_dir : str\n",
    "        Directory path to save the generated plot\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Convert y_test to binary format (ensures shape is correct)\n",
    "    y_bin = label_binarize(y_test, classes=[0, 1]).ravel()\n",
    "\n",
    "    # Step 2: Calculate precision and recall values for class 1\n",
    "    precision_1, recall_1, _ = precision_recall_curve(y_bin, y_probs)\n",
    "\n",
    "    # Step 3: Calculate average precision (area under PR curve)\n",
    "    ap_1 = average_precision_score(y_bin, y_probs)\n",
    "\n",
    "    # Step 4: Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(recall_1, precision_1, label=f\"Class 1 (AP = {ap_1:.4f})\", color='blue')\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"Precision-Recall Curve - {model_name}\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Step 5: Save figure\n",
    "    plt.savefig(os.path.join(save_dir, f\"PRC_{model_name}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd0fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === File Paths ===\n",
    "\n",
    "# Root directory where all extracted EEG feature datasets are stored\n",
    "data_dir = \"/Users/myatpwintphyu/Desktop/EEG Feature Extract Result\"\n",
    "\n",
    "# Directory where model results (plots, reports, etc.) will be saved\n",
    "result_root_dir = \"/Users/myatpwintphyu/Desktop/Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de440fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === List of EEG Feature Dataset Subfolders ===\n",
    "\n",
    "# These folders should exist under `data_dir`, and each contains `.npy` files for features and labels\n",
    "dataset_files = [\n",
    "    \"eeg_1dcnn_features\",        # Features extracted using 1D CNN\n",
    "    \"eeg_3dcnn_features\",        # Features extracted using 3D CNN\n",
    "    \"eeg_stft_beta_features\",    # STFT-based beta-band power features\n",
    "    \"eeg_stft_features\"          # Full-band STFT features across all common EEG bands\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ca4918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Models to Evaluate\n",
    "# Dictionary of model name → sklearn-compatible classifier instance\n",
    "models = {\n",
    "    \"Naive Bayes\": GaussianNB(),  # Simple probabilistic model based on Bayes' theorem\n",
    "\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=500),  \n",
    "    # Linear model for binary classification (can also handle multiclass with softmax)\n",
    "\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=2),  \n",
    "    # k-Nearest Neighbors with k=2 (you can tune this later)\n",
    "\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'),  \n",
    "    # Gradient boosting decision tree model (powerful for structured data)\n",
    "\n",
    "    \"MLP\": MLPClassifier(max_iter=1000),  \n",
    "    # Multi-layer Perceptron (neural network) with 1000 training iterations\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(),  \n",
    "    # Ensemble of decision trees (randomly sampled features and data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b631f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loop through all EEG feature datasets and evaluate models ===\n",
    "\n",
    "# Lists to store results across all datasets\n",
    "all_results = []\n",
    "all_report_rows = []\n",
    "\n",
    "# Iterate over each EEG feature folder\n",
    "for folder_name in dataset_files:\n",
    "    dataset_name = folder_name.replace(\"eeg_\", \"\")  # Clean dataset name\n",
    "    dataset_path = os.path.join(data_dir, folder_name)\n",
    "\n",
    "    # Create folders for saving results (PCA plots, PRC plots, etc.)\n",
    "    dataset_result_dir = os.path.join(result_root_dir, dataset_name)\n",
    "    os.makedirs(dataset_result_dir, exist_ok=True)\n",
    "    pca_dir = os.path.join(dataset_result_dir, \"PCA\")\n",
    "    prc_dir = os.path.join(dataset_result_dir, \"PRC\")\n",
    "    os.makedirs(pca_dir, exist_ok=True)\n",
    "    os.makedirs(prc_dir, exist_ok=True)\n",
    "\n",
    "    # === Load all feature chunks (X and y) ===\n",
    "    X_chunks, y_chunks = [], []\n",
    "    for fname in os.listdir(dataset_path):\n",
    "        if fname.startswith(\"X_feats_\"):\n",
    "            chunk_id = fname.split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "            X = np.load(os.path.join(dataset_path, f\"X_feats_{chunk_id}.npy\"))\n",
    "            y = np.load(os.path.join(dataset_path, f\"y_labels_{chunk_id}.npy\"))\n",
    "            X_chunks.append(X)\n",
    "            y_chunks.append(y)\n",
    "\n",
    "    # === Align all feature chunks to the same dimension (truncate or pad) ===\n",
    "    target_dim = min([x.shape[1] for x in X_chunks])  # Smallest feature dimension\n",
    "    X_aligned = []\n",
    "    for x in X_chunks:\n",
    "        if x.shape[1] > target_dim:\n",
    "            x_trimmed = x[:, :target_dim]  # Truncate\n",
    "        elif x.shape[1] < target_dim:\n",
    "            x_trimmed = np.pad(x, ((0, 0), (0, target_dim - x.shape[1])), mode='constant')  # Pad\n",
    "        else:\n",
    "            x_trimmed = x\n",
    "        X_aligned.append(x_trimmed)\n",
    "\n",
    "    # Combine all chunks into full dataset\n",
    "    X_all = np.vstack(X_aligned)\n",
    "    y_all = np.concatenate(y_chunks)\n",
    "\n",
    "    # === Split into train, validation, and test sets ===\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.4, stratify=y_all, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "    # === Normalize features ===\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # === Evaluate each ML model ===\n",
    "    results = []        # For summary (Accuracy, MAP)\n",
    "    report_rows = []    # For classification report\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Calculate metrics\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        map_score = average_precision_score(y_test, y_proba) if y_proba is not None else None\n",
    "\n",
    "        # Store summary metrics\n",
    "        results.append({\n",
    "            'Feature Set': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': acc,\n",
    "            'MAP': map_score\n",
    "        })\n",
    "\n",
    "        # Print quick summary\n",
    "        print(f\"✅ Feature Set: {dataset_name} | Model: {model_name} | Accuracy: {acc:.4f} | MAP: {map_score:.4f}\")\n",
    "\n",
    "        # Store classification report details\n",
    "        report = classification_report(y_test, y_pred, output_dict=True)\n",
    "        for cls, metrics in report.items():\n",
    "            if isinstance(metrics, dict):  # Skip 'accuracy' scalar\n",
    "                report_rows.append({\n",
    "                    'Feature Set': dataset_name,\n",
    "                    'Model': model_name,\n",
    "                    'Class': cls,\n",
    "                    'Precision': metrics.get('precision'),\n",
    "                    'Recall': metrics.get('recall'),\n",
    "                    'F1-Score': metrics.get('f1-score'),\n",
    "                    'Support': metrics.get('support')\n",
    "                })\n",
    "\n",
    "        # Plot PCA decision boundary\n",
    "        try:\n",
    "            plot_pca_decision_boundary(X_test, y_test, model, model_name, pca_dir)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ PCA plot error for {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "        # Plot precision-recall curve\n",
    "        if y_proba is not None:\n",
    "            try:\n",
    "                plot_precision_recall(y_test, y_proba, model_name, prc_dir)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ PRC plot error for {model_name} on {dataset_name}: {e}\")\n",
    "\n",
    "    # Aggregate results from this dataset\n",
    "    all_results.extend(results)\n",
    "    all_report_rows.extend(report_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ddc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ✅ Save final evaluation results after processing all EEG datasets ===\n",
    "\n",
    "# Convert the collected results into DataFrames\n",
    "final_results_df = pd.DataFrame(all_results)         # Summary: accuracy + MAP\n",
    "final_report_df = pd.DataFrame(all_report_rows)      # Detailed classification report\n",
    "\n",
    "# Save both results as CSV files to the results folder\n",
    "final_results_df.to_csv(os.path.join(result_root_dir, \"all_model_summary.csv\"), index=False)\n",
    "final_report_df.to_csv(os.path.join(result_root_dir, \"all_classification_report.csv\"), index=False)\n",
    "\n",
    "# Final status message\n",
    "print(\"✅ All model evaluations complete. Combined CSVs saved to Final_CV_Only_Results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97c9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🔧 Step 2: Define hyperparameter search space for XGBoost ===\n",
    "# Each key corresponds to a tunable hyperparameter, and the value is the allowed range\n",
    "\n",
    "param_space = {\n",
    "    'learning_rate': (0.05, 0.2),         # Controls how quickly the model adapts (smaller = slower, more stable)\n",
    "    'max_depth': (3, 6),                  # Maximum depth of trees (controls model complexity)\n",
    "    'n_estimators': (100, 300),           # Number of boosting rounds (more = potentially better fit, but slower)\n",
    "    'subsample': (0.7, 1.0),              # Fraction of training data used per tree (helps prevent overfitting)\n",
    "    'colsample_bytree': (0.7, 1.0)        # Fraction of features used per tree (adds diversity)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa30d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🔧 Model Configuration Dictionary ===\n",
    "# This setup maps model names to:\n",
    "# (1) a lambda function that creates the model with given parameters,\n",
    "# (2) the corresponding hyperparameter search space.\n",
    "\n",
    "model_configs = {\n",
    "    'XGBoost': (\n",
    "        # 👇 Lambda returns an XGBoost classifier with tuned parameters injected\n",
    "        lambda **params: XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='mlogloss',        # Standard for binary/multiclass classification\n",
    "            scale_pos_weight=1.2604,       # Optional: adjusts for class imbalance (e.g., depression cases)\n",
    "            random_state=42,               # Ensures reproducibility\n",
    "            **params                       # Parameters passed by optimizer\n",
    "        ),\n",
    "        {\n",
    "            # 🔍 Define the search space for hyperparameter optimization\n",
    "            'learning_rate': (0.05, 0.2),       # Step size shrinkage (smaller = more stable)\n",
    "            'max_depth': (3, 6),                # Tree depth (controls complexity)\n",
    "            'n_estimators': (100, 300),         # Number of boosting rounds\n",
    "            'subsample': (0.7, 1.0),            # Fraction of training data used in each boosting round\n",
    "            'colsample_bytree': (0.7, 1.0)      # Fraction of features used per tree\n",
    "        }\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fe9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ⚙️ Available Optimizers ===\n",
    "# Dictionary mapping names to their respective optimizer classes.\n",
    "# These optimizers will be used to tune model hyperparameters.\n",
    "\n",
    "optimizers = {\n",
    "    'JASA': JASAOptimizer,             # Jellyfish Algorithm with Simulated Annealing\n",
    "    'DHOA-SA': DHOA_SA_Optimizer,      # Dynamic Harris Hawk Optimization with SA\n",
    "    'IWO-SA': IWO_SA_Optimizer,        # Invasive Weed Optimization with SA\n",
    "    'COS': COSOptimizer,               # Crisscross Optimization Strategy\n",
    "    'COIWSO-SA': COIWSO_SA_Optimizer   # Hybrid of COS and IWO with SA\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66124c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🧠 Dynamic Optimizer Builder ===\n",
    "# This function constructs and returns an optimizer instance,\n",
    "# injecting only the relevant arguments based on the optimizer's constructor.\n",
    "\n",
    "def build_optimizer(OptClass, model_class, param_space_used):\n",
    "    # 🧰 Default arguments for all optimizers (some may ignore extra ones)\n",
    "    kwargs = {\n",
    "        'model_class': model_class,             # Model to be optimized (e.g., XGBoost constructor)\n",
    "        'param_space': param_space_used,        # Search space for hyperparameters\n",
    "        'max_iter': 30,                         # Number of optimization iterations\n",
    "        'population_size': 10,                  # For population-based optimizers (e.g., IWO)\n",
    "        'initial_pop': 5,                       # Starting population\n",
    "        'max_pop': 15,                          # Max growth of population (IWO-related)\n",
    "        'swarm_size': 5,                        # Swarm size (if applicable)\n",
    "        'temperature': 1.0,                     # Initial temp for simulated annealing\n",
    "        'cooling_rate': 0.95,                   # Cooling schedule\n",
    "        'mutation_rate': 0.1,                   # Mutation for evolutionary optimizers\n",
    "        'metric_to_optimize': 'accuracy'        # Main metric for fitness evaluation\n",
    "    }\n",
    "\n",
    "    # 🧼 Filter out any arguments not accepted by this optimizer class\n",
    "    sig = inspect.signature(OptClass.__init__)\n",
    "    valid_args = {k: v for k, v in kwargs.items() if k in sig.parameters}\n",
    "\n",
    "    # 🛠️ Build and return the optimizer instance\n",
    "    return OptClass(**valid_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ea0946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 🚀 Start Optimization Across All Datasets ===\n",
    "\n",
    "optimizer_summary = []           # 📊 Store best result from each iteration per optimizer\n",
    "final_optimizer_results = []     # 🏆 Store final best config (lowest cost + best acc + MAP)\n",
    "\n",
    "# === Loop through each dataset (1D CNN, 3D CNN, STFT beta, STFT full) ===\n",
    "for folder_name in dataset_files:\n",
    "    dataset_name = folder_name.replace(\"eeg_\", \"\")\n",
    "    dataset_path = os.path.join(data_dir, folder_name)\n",
    "\n",
    "    # 📁 Create result folders\n",
    "    dataset_result_dir = os.path.join(result_root_dir, dataset_name)\n",
    "    pca_dir = os.path.join(dataset_result_dir, \"PCA\")\n",
    "    prc_dir = os.path.join(dataset_result_dir, \"PRC\")\n",
    "    os.makedirs(pca_dir, exist_ok=True)\n",
    "    os.makedirs(prc_dir, exist_ok=True)\n",
    "\n",
    "    # 📥 Load feature chunks\n",
    "    X_chunks, y_chunks = [], []\n",
    "    for fname in os.listdir(dataset_path):\n",
    "        if fname.startswith(\"X_feats_\"):\n",
    "            chunk_id = fname.split(\"_\")[-1].replace(\".npy\", \"\")\n",
    "            X = np.load(os.path.join(dataset_path, f\"X_feats_{chunk_id}.npy\"))\n",
    "            y = np.load(os.path.join(dataset_path, f\"y_labels_{chunk_id}.npy\"))\n",
    "            X_chunks.append(X)\n",
    "            y_chunks.append(y)\n",
    "\n",
    "    # 🔄 Align all feature sets to the same number of dimensions\n",
    "    target_dim = min([x.shape[1] for x in X_chunks])\n",
    "    X_aligned = []\n",
    "    for x in X_chunks:\n",
    "        if x.shape[1] > target_dim:\n",
    "            x_trimmed = x[:, :target_dim]\n",
    "        elif x.shape[1] < target_dim:\n",
    "            x_trimmed = np.pad(x, ((0, 0), (0, target_dim - x.shape[1])), mode='constant')\n",
    "        else:\n",
    "            x_trimmed = x\n",
    "        X_aligned.append(x_trimmed)\n",
    "\n",
    "    # 🔗 Combine all features & labels\n",
    "    X_all = np.vstack(X_aligned)\n",
    "    y_all = np.concatenate(y_chunks)\n",
    "\n",
    "    # 📊 Split data into Train / Validation / Test sets\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.4, stratify=y_all, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "    # 🧼 Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 📦 Get XGBoost model and its parameter space\n",
    "    model_builder, param_space = model_configs[\"XGBoost\"]\n",
    "\n",
    "    # === 🧪 Loop through each optimizer ===\n",
    "    for opt_name, OptClass in optimizers.items():\n",
    "        print(f\"\\n🔧 Optimizer: {opt_name} on {dataset_name}\")\n",
    "\n",
    "        # 🔨 Build and run the optimizer\n",
    "        optimizer = build_optimizer(OptClass, model_builder, param_space)\n",
    "        optimizer.fit(X_train, y_train, X_test, y_test)\n",
    "\n",
    "        # 📈 Track all iterations\n",
    "        df_full = pd.DataFrame(optimizer.history)\n",
    "\n",
    "        # 🏅 Select best candidate per iteration based on:\n",
    "        # 1. Lowest cost ➝ 2. Highest accuracy ➝ 3. Highest MAP\n",
    "        best_rows = []\n",
    "        for iter_id, group in df_full.groupby(\"iteration\"):\n",
    "            min_cost = group[\"Cost Function\"].min()\n",
    "            lowest_cost_rows = group[group[\"Cost Function\"] == min_cost]\n",
    "            max_acc = lowest_cost_rows[\"accuracy\"].max()\n",
    "            best_acc_rows = lowest_cost_rows[lowest_cost_rows[\"accuracy\"] == max_acc]\n",
    "            best_row = best_acc_rows.loc[best_acc_rows[\"MAP\"].idxmax()]\n",
    "            best_row_dict = best_row.to_dict()\n",
    "            best_row_dict[\"iteration\"] = iter_id\n",
    "            best_rows.append(best_row_dict)\n",
    "\n",
    "        # 🗂️ Store best per iteration\n",
    "        df = pd.DataFrame(best_rows).reset_index(drop=True)\n",
    "        df.insert(0, \"Feature Set\", dataset_name)\n",
    "        df.insert(1, \"Optimizer\", opt_name)\n",
    "\n",
    "        # 📑 Only keep relevant columns\n",
    "        keep_cols = [\n",
    "            \"Feature Set\", \"Optimizer\", \"iteration\", \"learning_rate\", \"max_depth\", \"n_estimators\",\n",
    "            \"subsample\", \"colsample_bytree\", \"accuracy\", \"MAR\", \"FDR\", \"FPR\", \"MCC\",\n",
    "            \"precision\", \"MAP\", \"Cost Function\"\n",
    "        ]\n",
    "        df = df[[col for col in keep_cols if col in df.columns]]\n",
    "        optimizer_summary.append(df)\n",
    "\n",
    "        # 🎯 Select final best config (lowest cost ➝ highest acc ➝ highest MAP)\n",
    "        best_cost = df[\"Cost Function\"].min()\n",
    "        lowest_cost_df = df[df[\"Cost Function\"] == best_cost]\n",
    "        max_acc = lowest_cost_df[\"accuracy\"].max()\n",
    "        highest_acc_df = lowest_cost_df[lowest_cost_df[\"accuracy\"] == max_acc]\n",
    "        final_best_row = highest_acc_df.loc[highest_acc_df[\"MAP\"].idxmax()]\n",
    "        final_optimizer_results.append(final_best_row)\n",
    "\n",
    "        # 📊 Accuracy per iteration plot\n",
    "        fig_acc, ax_acc = plt.subplots()\n",
    "        ax_acc.plot(optimizer.best_accuracy_per_iteration)\n",
    "        ax_acc.set_title(f\"{opt_name} Accuracy on {dataset_name}\")\n",
    "        ax_acc.set_xlabel(\"Iteration\")\n",
    "        ax_acc.set_ylabel(\"Accuracy\")\n",
    "        ax_acc.grid(True)\n",
    "        fig_acc.tight_layout()\n",
    "        fig_acc.savefig(os.path.join(dataset_result_dir, f\"{opt_name}_accuracy_plot.png\"))\n",
    "        plt.close(fig_acc)\n",
    "\n",
    "        # 📉 Cost function per iteration plot\n",
    "        fig_cost, ax_cost = plt.subplots()\n",
    "        ax_cost.plot(optimizer.best_cost_per_iteration)\n",
    "        ax_cost.set_title(f\"{opt_name} Cost Function on {dataset_name}\")\n",
    "        ax_cost.set_xlabel(\"Iteration\")\n",
    "        ax_cost.set_ylabel(\"Cost\")\n",
    "        ax_cost.grid(True)\n",
    "        fig_cost.tight_layout()\n",
    "        fig_cost.savefig(os.path.join(dataset_result_dir, f\"{opt_name}_cost_plot.png\"))\n",
    "        plt.close(fig_cost)\n",
    "\n",
    "# === 💾 Save Final Results ===\n",
    "summary_df = pd.concat(optimizer_summary, ignore_index=True)\n",
    "summary_df.to_csv(os.path.join(result_root_dir, \"all_optimizer_summary.csv\"), index=False)\n",
    "\n",
    "final_df = pd.DataFrame(final_optimizer_results)\n",
    "final_df.to_csv(os.path.join(result_root_dir, \"final_optimizer_results.csv\"), index=False)\n",
    "\n",
    "print(\"✅ All optimizer results saved:\")\n",
    "print(\" - all_optimizer_summary.csv (best per iteration)\")\n",
    "print(\" - final_optimizer_results.csv (final best only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec42de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# === Paths to final CSVs ===\n",
    "final_result_path = os.path.join(result_root_dir, \"final_optimizer_results.csv\")\n",
    "all_iter_path = os.path.join(result_root_dir, \"all_optimizer_summary.csv\")\n",
    "\n",
    "# === Load CSVs ===\n",
    "final_results = pd.read_csv(final_result_path)\n",
    "summary_df = pd.read_csv(all_iter_path)\n",
    "\n",
    "# === Metrics to visualize\n",
    "metrics_to_plot = {\n",
    "    \"accuracy\": \"Accuracy (Higher is Better)\",\n",
    "    \"Cost Function\": \"Cost Function (Lower is Better)\",\n",
    "    \"MAP\": \"Mean Average Precision (MAP)\"\n",
    "}\n",
    "\n",
    "# === Loop through each EEG dataset (feature set)\n",
    "for feature_set in final_results[\"Feature Set\"].unique():\n",
    "    print(f\"\\n📊 Generating visualizations for: {feature_set}\")\n",
    "\n",
    "    # Filter for this feature set\n",
    "    subset = final_results[final_results[\"Feature Set\"] == feature_set]\n",
    "    subset_iter = summary_df[summary_df[\"Feature Set\"] == feature_set]\n",
    "\n",
    "    # Create output folder\n",
    "    dataset_dir = os.path.join(result_root_dir, feature_set)\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # === 1. Bar Charts for accuracy, cost, MAP ===\n",
    "    for metric, title in metrics_to_plot.items():\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sorted_subset = subset.sort_values(metric, ascending=(metric == \"Cost Function\"))\n",
    "\n",
    "        ax = sns.barplot(\n",
    "            x=\"Optimizer\",\n",
    "            y=metric,\n",
    "            data=sorted_subset,\n",
    "            palette=\"viridis\"\n",
    "        )\n",
    "\n",
    "        # Annotate bars with value\n",
    "        for bar in ax.patches:\n",
    "            height = bar.get_height()\n",
    "            ax.text(\n",
    "                bar.get_x() + bar.get_width() / 2,\n",
    "                height + 0.002,\n",
    "                f\"{height:.3f}\",\n",
    "                ha='center',\n",
    "                va='bottom',\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "        plt.title(f\"{title} – {feature_set.capitalize()}\", fontsize=12)\n",
    "        plt.xlabel(\"Optimizer\", fontsize=10)\n",
    "        plt.ylabel(title, fontsize=10)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        bar_path = os.path.join(dataset_dir, f\"{metric.replace(' ', '_')}_bar_chart.png\")\n",
    "        plt.savefig(bar_path)\n",
    "        plt.close()\n",
    "        print(f\"✅ Saved: {bar_path}\")\n",
    "\n",
    "    # === 2. Bubble Chart (Simple View) ===\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scaled_size = (subset['MAP'] - 0.927) * 900000\n",
    "    scaled_size = np.clip(scaled_size, 50, 3000)\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        subset['Cost Function'],\n",
    "        subset['accuracy'],\n",
    "        s=scaled_size,\n",
    "        c=subset['MAP'],\n",
    "        cmap='viridis',\n",
    "        alpha=0.7,\n",
    "        edgecolors='w',\n",
    "        linewidths=1.5\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Cost Function (Lower is Better)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (Higher is Better)', fontsize=14)\n",
    "    plt.title(f'Bubble Chart: {feature_set.capitalize()}', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('MAP Score', fontsize=12)\n",
    "\n",
    "    bubble_path = os.path.join(dataset_dir, f\"bubble_chart_simple.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(bubble_path)\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {bubble_path}\")\n",
    "\n",
    "    # === 3. Bubble Chart (Fancy with Legend) ===\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    norm = plt.Normalize(vmin=subset['MAP'].min(), vmax=subset['MAP'].max())\n",
    "    colors = plt.cm.plasma(norm(subset['MAP']))\n",
    "\n",
    "    scatter = plt.scatter(\n",
    "        subset['Cost Function'],\n",
    "        subset['accuracy'],\n",
    "        s=scaled_size,\n",
    "        c=colors,\n",
    "        alpha=0.8,\n",
    "        edgecolors='w',\n",
    "        linewidths=1.5\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Cost Function (Lower is Better)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (Higher is Better)', fontsize=14)\n",
    "    plt.title(f'{feature_set.capitalize()} - Optimizer Bubble Chart', fontsize=16)\n",
    "    plt.grid(True)\n",
    "\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('MAP Score', fontsize=12)\n",
    "\n",
    "    # Add optimizer legend manually\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(\n",
    "            facecolor=colors[i],\n",
    "            edgecolor='black',\n",
    "            label=f\"{row['Optimizer']} (MAP={row['MAP']:.3f})\"\n",
    "        )\n",
    "        for i, (_, row) in enumerate(subset.iterrows())\n",
    "    ]\n",
    "\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        title=\"Optimizer\",\n",
    "        loc='upper center',\n",
    "        bbox_to_anchor=(0.5, -0.18),\n",
    "        ncol=3,\n",
    "        frameon=False\n",
    "    )\n",
    "\n",
    "    fancy_path = os.path.join(dataset_dir, f\"bubble_chart_fancy.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fancy_path)\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {fancy_path}\")\n",
    "\n",
    "    # === 4. Step Plot for Cost Function over Iterations ===\n",
    "    cost_data = subset_iter[['iteration', 'Optimizer', 'Cost Function']].copy()\n",
    "    marker_positions = np.arange(5, cost_data['iteration'].max() + 1, 5)\n",
    "    optimizers = cost_data['Optimizer'].unique()\n",
    "    palette = sns.color_palette('tab10', len(optimizers))\n",
    "    color_dict = dict(zip(optimizers, palette))\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for optimizer, group in cost_data.groupby('Optimizer'):\n",
    "        plt.plot(\n",
    "            group['iteration'],\n",
    "            group['Cost Function'],\n",
    "            label=optimizer,\n",
    "            color=color_dict[optimizer],\n",
    "            drawstyle='steps-post',\n",
    "            linewidth=2\n",
    "        )\n",
    "        marker_group = group[group['iteration'].isin(marker_positions)]\n",
    "        plt.scatter(\n",
    "            marker_group['iteration'],\n",
    "            marker_group['Cost Function'],\n",
    "            color=color_dict[optimizer],\n",
    "            s=20,\n",
    "            zorder=5\n",
    "        )\n",
    "\n",
    "    plt.title(f'{feature_set.capitalize()} - Optimizer Cost Function Progress', fontsize=16)\n",
    "    plt.xlabel('Iteration', fontsize=14)\n",
    "    plt.ylabel('Cost Function', fontsize=14)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', title='Optimizer', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    step_path = os.path.join(dataset_dir, f\"cost_function_step_plot.png\")\n",
    "    plt.savefig(step_path)\n",
    "    plt.close()\n",
    "    print(f\"✅ Saved: {step_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# === Load the optimizer summary CSV ===\n",
    "file_path = \"/Users/myatpwintphyu/Desktop/Results/all_optimizer_summary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === Mapping short optimizer codes to readable labels for legends ===\n",
    "optimizer_label_map = {\n",
    "    \"JASA\": \"Jellyfish\",\n",
    "    \"DHOA-SA\": \"DHOA-SA\",\n",
    "    \"COS\": \"Crisscross\",\n",
    "    \"IWO-SAO\": \"Weed-SA\",         # You may want to double-check this spelling (should it be \"IWO-SA\"?)\n",
    "    \"COIWSO-SA\": \"COIWSO-SA\"\n",
    "}\n",
    "\n",
    "# === High-contrast color scheme for distinguishable lines ===\n",
    "colors = {\n",
    "    \"JASA\": \"#E41A1C\",       # Red\n",
    "    \"DHOA-SA\": \"#377EB8\",    # Blue\n",
    "    \"COS\": \"#4DAF4A\",        # Green\n",
    "    \"COIWSO-SA\": \"#FF7F00\",  # Orange\n",
    "    \"IWO-SA\": \"#984EA3\",     # Purple\n",
    "}\n",
    "\n",
    "# === Identify the unique EEG datasets (feature sets) ===\n",
    "feature_sets = df[\"Feature Set\"].unique()\n",
    "\n",
    "# === Create subplots: 2 rows × 2 columns ===\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()  # Make it easier to loop through\n",
    "\n",
    "# === Plot cost function trends for the first 4 feature sets ===\n",
    "for idx, feature_set in enumerate(feature_sets[:4]):\n",
    "    ax = axes[idx]\n",
    "    subset = df[df[\"Feature Set\"] == feature_set]\n",
    "    \n",
    "    # Plot each optimizer's progress over iterations\n",
    "    for optimizer in subset[\"Optimizer\"].unique():\n",
    "        opt_data = subset[subset[\"Optimizer\"] == optimizer]\n",
    "        ax.plot(\n",
    "            opt_data[\"iteration\"],\n",
    "            opt_data[\"Cost Function\"],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            markersize=5,\n",
    "            label=optimizer_label_map.get(optimizer, optimizer),   # Fallback if label missing\n",
    "            color=colors.get(optimizer, \"gray\")                   # Fallback to gray if color missing\n",
    "        )\n",
    "    \n",
    "    # Add subplot title and axes labels\n",
    "    ax.set_title(f\"({chr(97 + idx)}) {feature_set}\", loc='left', fontsize=13, fontweight='bold')\n",
    "    ax.set_xlabel(\"Iterations\", fontsize=12)\n",
    "    ax.set_ylabel(\"Cost Function\", fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.5)\n",
    "    ax.legend(fontsize=10)\n",
    "\n",
    "# === Final layout adjustment and display ===\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# === Load optimizer summary CSV ===\n",
    "file_path = \"/Users/myatpwintphyu/Desktop/Results/all_optimizer_summary.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# === Map internal optimizer names to readable labels ===\n",
    "optimizer_label_map = {\n",
    "    \"JASA\": \"Jellyfish\",\n",
    "    \"DHOA-SA\": \"DHOA-SA\",\n",
    "    \"COS\": \"Crisscross\",\n",
    "    \"IWO-SAO\": \"Weed-SA\",         # Double-check spelling: may need to fix \"IWO-SA\"\n",
    "    \"COIWSO-SA\": \"COIWSO-SA\"\n",
    "}\n",
    "\n",
    "# === Assign high-contrast colors to each optimizer ===\n",
    "colors = {\n",
    "    \"JASA\": \"#E41A1C\",       # Bright red\n",
    "    \"DHOA-SA\": \"#377EB8\",    # Blue\n",
    "    \"COS\": \"#4DAF4A\",        # Green\n",
    "    \"COIWSO-SA\": \"#FF7F00\",  # Orange\n",
    "    \"IWO-SA\": \"#984EA3\",     # Purple\n",
    "}\n",
    "\n",
    "# === Create folder for separate plots ===\n",
    "output_dir = \"/Users/myatpwintphyu/Desktop/Results/Separate_Figures\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# === Generate one plot per feature set ===\n",
    "feature_sets = df[\"Feature Set\"].unique()\n",
    "\n",
    "for feature_set in feature_sets:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    subset = df[df[\"Feature Set\"] == feature_set]\n",
    "\n",
    "    # Plot each optimizer’s cost curve\n",
    "    for optimizer in subset[\"Optimizer\"].unique():\n",
    "        opt_data = subset[subset[\"Optimizer\"] == optimizer]\n",
    "        plt.plot(\n",
    "            opt_data[\"iteration\"],\n",
    "            opt_data[\"Cost Function\"],\n",
    "            marker='o',\n",
    "            linewidth=2,\n",
    "            markersize=5,\n",
    "            label=optimizer_label_map.get(optimizer, optimizer),  # fallback if not in map\n",
    "            color=colors.get(optimizer, \"gray\")                   # fallback color\n",
    "        )\n",
    "    \n",
    "    # Format the plot\n",
    "    plt.xlabel(\"Iterations\", fontsize=12)\n",
    "    plt.ylabel(\"Cost Function\", fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure with a clean filename\n",
    "    save_path = os.path.join(output_dir, f\"{feature_set.replace(' ', '_')}_Cost_Function_Analysis.png\")\n",
    "    plt.savefig(save_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "print(\"✅ All separate plots saved to:\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
